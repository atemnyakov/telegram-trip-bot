import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset


class AnyDestinationClassifier:
    def __init__(self, path: str = f'./models/AnyDestinationClassifier'):
        self.path: str = path
        self.model: BertForSequenceClassification | None = None
        self.tokenizer: BertTokenizer | None = None

    def learn(self) -> None:
        # 1ï¸âƒ£ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ð¹ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
        self.tokenizer = BertTokenizer.from_pretrained("cointegrated/rubert-tiny")

        # 2ï¸âƒ£ Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð²Ð¸Ð´Ðµ ÑÐ»Ð¾Ð²Ð°Ñ€Ñ {Ñ‚ÐµÐºÑÑ‚: Ð»ÐµÐ¹Ð±Ð»}
        train_data = {
            "Ð¥Ð¾Ñ‡Ñƒ Ð¿Ð¾Ð»ÐµÑ‚ÐµÑ‚ÑŒ ÐºÑƒÐ´Ð° ÑƒÐ³Ð¾Ð´Ð½Ð¾": 1,
            "Ð“Ð´Ðµ Ð´ÐµÑˆÑ‘Ð²Ñ‹Ðµ Ð±Ð¸Ð»ÐµÑ‚Ñ‹ Ð² Ð»ÑŽÐ±ÑƒÑŽ Ñ‚Ð¾Ñ‡ÐºÑƒ?": 1,
            "Ð›ÑŽÐ±Ð°Ñ ÑÑ‚Ñ€Ð°Ð½Ð°, Ð¼Ð½Ðµ Ð²ÑÑ‘ Ñ€Ð°Ð²Ð½Ð¾": 1,
            "ÐšÑƒÐ´Ð°-Ñ‚Ð¾, Ð½Ð¾ Ð½Ðµ Ð·Ð½Ð°ÑŽ ÐºÑƒÐ´Ð°": 1,
            "Ð’ Ð»ÑŽÐ±ÑƒÑŽ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñƒ, Ð»Ð¸ÑˆÑŒ Ð±Ñ‹ ÑƒÐ»ÐµÑ‚ÐµÑ‚ÑŒ": 1,
            "Ð¥Ð¾Ñ‡Ñƒ Ð¿Ð¾Ð»ÐµÑ‚ÐµÑ‚ÑŒ Ð¸Ð· ÐŸÑ€Ð°Ð³Ð¸ ÐºÑƒÐ´Ð°-Ñ‚Ð¾ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ…": 1,
            "Ð¥Ð¾Ñ‡Ñƒ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ… ÑƒÐ»ÐµÑ‚ÐµÑ‚ÑŒ ÐºÑƒÐ´Ð°-Ñ‚Ð¾": 1,
            "ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ Ð¼Ð½Ðµ Ð¿ÑƒÑ‚ÐµÑˆÐµÑÑ‚Ð²Ð¸Ñ Ð¸Ð· ÐŸÑ€Ð°Ð³Ð¸ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð¾ 2000 ÐºÑ€Ð¾Ð½": 1,

            "Ð¥Ð¾Ñ‡Ñƒ Ð² ÐŸÐ°Ñ€Ð¸Ð¶": 0,
            "Ð˜Ñ‰Ñƒ Ð±Ð¸Ð»ÐµÑ‚Ñ‹ Ð² Ð›Ð¾Ð½Ð´Ð¾Ð½": 0,
            "Ð“Ð´Ðµ Ð´ÐµÑˆÑ‘Ð²Ñ‹Ðµ Ð±Ð¸Ð»ÐµÑ‚Ñ‹ Ð² Ð¢Ð¾ÐºÐ¸Ð¾?": 0,
            "ÐœÐ½Ðµ Ð½ÑƒÐ¶ÐµÐ½ Ð±Ð¸Ð»ÐµÑ‚ Ð² ÐœÐ¾ÑÐºÐ²Ñƒ": 0,
            "ÐšÐ°Ðº ÑƒÐ»ÐµÑ‚ÐµÑ‚ÑŒ Ð² ÐÑŒÑŽ-Ð™Ð¾Ñ€Ðº?": 0,
            "ÐÐ°Ð¹Ð´Ð¸ Ð±Ð¸Ð»ÐµÑ‚Ñ‹ Ð¼Ð½Ðµ Ð¸Ð· Ð›Ð¾Ð½Ð´Ð¾Ð½Ð° Ð´Ð¾ Ð Ð¸Ð¼Ð°": 0
        }

        # 3ï¸âƒ£ ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð² ÑÐ¿Ð¸ÑÐºÐ¸
        train_texts = list(train_data.keys())  # Ð¢ÐµÐºÑÑ‚Ñ‹ (ÑÐ¿Ð¸ÑÐ¾Ðº)
        train_labels = list(train_data.values())  # ÐœÐµÑ‚ÐºÐ¸ (ÑÐ¿Ð¸ÑÐ¾Ðº)

        # 4ï¸âƒ£ Ð¢Ð¾ÐºÐµÐ½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼
        train_encodings = self.tokenizer(train_texts, truncation=True, padding=True, max_length=32, return_tensors="pt")

        # 5ï¸âƒ£ ÐšÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ð¹ Dataset
        class FlightDataset(Dataset):
            def __init__(self, encodings, labels):
                self.encodings = encodings
                self.labels = labels

            def __len__(self):
                return len(self.labels)

            def __getitem__(self, idx):
                item = {key: val[idx] for key, val in self.encodings.items()}
                item["labels"] = torch.tensor(self.labels[idx])
                return item

        train_dataset = FlightDataset(train_encodings, train_labels)

        # 6ï¸âƒ£ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ BERT
        self.model = BertForSequenceClassification.from_pretrained("cointegrated/rubert-tiny", num_labels=2)

        # 7ï¸âƒ£ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
        training_args = TrainingArguments(
            output_dir="../results",
            num_train_epochs=10,
            per_device_train_batch_size=2,
            logging_dir="./logs",
            logging_steps=1,
            save_strategy="no"
        )

        # 8ï¸âƒ£ ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
        )

        trainer.train()

    def save(self) -> None:
        # Save model and tokenizer
        self.model.save_pretrained(self.path)
        self.tokenizer.save_pretrained(self.path)

    def load(self) -> None:
        # Load the trained model and tokenizer
        self.model = BertForSequenceClassification.from_pretrained(self.path)
        self.tokenizer = BertTokenizer.from_pretrained(self.path)

    def test(self) -> None:
        # 9ï¸âƒ£ Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼
        test_data = {
            "Ð¥Ð¾Ñ‡Ñƒ Ð² Ð‘Ð°Ñ€ÑÐµÐ»Ð¾Ð½Ñƒ": 0,
            "Ð•ÑÑ‚ÑŒ Ð»Ð¸ ÐºÐ°ÐºÐ¸Ðµ-Ñ‚Ð¾ Ð±Ð¸Ð»ÐµÑ‚Ñ‹ Ð² ÐœÐ¸Ð»Ð°Ð½ Ð¸Ð· ÐŸÑ€Ð°Ð³Ð¸ Ð·Ð° 50 ÐµÐ²Ñ€Ð¾ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ…?": 0,
            "ÐšÑƒÐ´Ð°-Ð½Ð¸Ð±ÑƒÐ´ÑŒ, Ð½ÐµÐ²Ð°Ð¶Ð½Ð¾ ÐºÑƒÐ´Ð°": 1,
            "Ð¯ Ñ…Ð¾Ñ‡Ñƒ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÐµÑ‚ÐµÑ‚ÑŒ ÐºÑƒÐ´Ð°-Ñ‚Ð¾": 1,
            "Ð¯ Ñ…Ð¾Ñ‡Ñƒ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÐµÑ‚ÐµÑ‚ÑŒ ÐºÑƒÐ´Ð°-Ñ‚Ð¾ Ð¿Ð¾ Ð•Ð²Ñ€Ð¾Ð¿Ðµ": 1,
            "ÐšÑƒÐ´Ð° Ñ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð»ÐµÑ‚ÐµÑ‚ÑŒ Ð¸Ð· ÐŸÑ€Ð°Ð³Ð¸ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð·Ð° 100 ÐµÐ²Ñ€Ð¾?": 1,
            "ÐšÑƒÐ´Ð° Ñ Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐ»ÐµÑ‚ÐµÑ‚ÑŒ Ð¸Ð· Ð’ÐµÐ½Ñ‹ Ð·Ð° Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ñ„Ð¾Ñ€Ð¸Ð½Ñ‚Ð¾Ð²?": 1,
        }

        test_texts = list(test_data.keys())
        test_labels = list(test_data.values())

        test_encodings = self.tokenizer(test_texts, truncation=True, padding=True, max_length=32, return_tensors="pt")

        self.model.eval()
        with torch.no_grad():
            outputs = self.model(**test_encodings)
            predictions = torch.argmax(outputs.logits, dim=-1)

        #  ðŸ”¥ Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
        print("Expected:", test_labels)
        print("Predicted:", predictions.tolist())

    def predict(self, text: str) -> bool:
        inputs = self.tokenizer(text, truncation=True, padding=True, max_length=32, return_tensors="pt")

        # Predict
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(**inputs)
            prediction = torch.argmax(outputs.logits, dim=-1).item()

        # Print result
        print("Prediction:", prediction)  # 1 (any destination) or 0 (specific destination)
        return prediction
